"""
WebRTC ì—°ê²° ë° ì˜¤ë””ì˜¤ ì²˜ë¦¬ í•¸ë“¤ëŸ¬
"""
import asyncio
import logging
import numpy as np
from typing import Optional, List, Callable
from collections import deque
from fastapi import WebSocket
from aiortc import RTCPeerConnection, RTCSessionDescription, RTCIceCandidate, MediaStreamTrack, RTCDataChannel
from aiortc.contrib.media import MediaPlayer, MediaRelay
from aiortc.sdp import candidate_from_sdp
import json
import wave
import os
import time
from datetime import datetime

from audio_encoder import AudioEncoder, encode_audio_frame_for_vad
from realtime_stt_client import RealtimeSttClient
from vad_segmenter import VADSegmenter
import webrtcvad

logger = logging.getLogger(__name__)


# AudioChunkBufferëŠ” AudioEncoder í´ë˜ìŠ¤ë¡œ ëŒ€ì²´ë¨


class AudioTrackReceiver(MediaStreamTrack):
    """WebRTCì—ì„œ ë°›ì€ ì˜¤ë””ì˜¤ íŠ¸ë™ì„ ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ (ë¡œì»¬ VAD ëª¨ë“œ: VADë¡œ ë§ ë ê°ì§€ í›„ commit)"""
    kind = "audio"
    
    def __init__(self, track, stt_client: Optional[RealtimeSttClient], mic_enabled_callback: Optional[Callable[[], bool]] = None, digital_gain_db: float = 6.0):
        super().__init__()
        self.track = track
        self.stt_client = stt_client  # STT í´ë¼ì´ì–¸íŠ¸
        self.mic_enabled_callback = mic_enabled_callback  # ë§ˆì´í¬ ìƒíƒœ í™•ì¸ ì½œë°±
        
        # ì˜¤ë””ì˜¤ ì¸ì½”ë” (ë²„í¼ ê¸°ë°˜, 24kHz, 20ms ì²­í¬)
        # digital_gain_db: ì…ë ¥ ìŒëŸ‰ ì¦ê°€ (peak 3000~15000 ë²”ìœ„ë¡œ ì¡°ì •)
        self.audio_encoder = AudioEncoder(digital_gain_db=digital_gain_db)
        
        # VAD (Voice Activity Detection) - 16kHzìš©
        self.vad = webrtcvad.Vad(2)  # ëª¨ë“œ 2 (0-3, 2ê°€ ì ë‹¹)
        
        # VADSegmenter (ë§ ë ê°ì§€ í›„ commit)
        self.vad_segmenter: Optional[VADSegmenter] = None
        if stt_client:
            self.vad_segmenter = VADSegmenter(
                on_clear=lambda: stt_client.clear(),
                on_append=lambda chunk: stt_client.append_audio(chunk),
                on_commit=lambda: stt_client.commit(),
                on_get_buffered_ms=lambda: stt_client.get_stats().get('buffered_ms', 0),
                hangover_ms=500,
                min_commit_ms=100
            )
        
        # í†µê³„ ì¶”ì  (5ì´ˆë§ˆë‹¤ ë¡œê·¸)
        self._stats = {
            'peak_sum': 0,
            'rms_sum': 0.0,
            'zero_ratio_sum': 0.0,
            'clipped_ratio_sum': 0.0,
            'chunk_count': 0,
            'last_log_time': time.time()
        }
        
        # WebRTC í”„ë ˆì„ ì •ë³´ ì¶”ì  (ì²« í”„ë ˆì„ë§Œ ìƒì„¸ ë¡œê·¸)
        self._first_frame_logged = False
        self._append_count = 0  # append í˜¸ì¶œ ì¹´ìš´í„°
        
    async def recv(self):
        """ì˜¤ë””ì˜¤ í”„ë ˆì„ ìˆ˜ì‹  - ì„œë²„ VAD ëª¨ë“œ: appendë§Œ ì—°ì† ì „ì†¡"""
        frame = await self.track.recv()
        
        # STT í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìœ¼ë©´ í”„ë ˆì„ë§Œ ë°˜í™˜
        if not self.stt_client:
            return frame
        
        # ë§ˆì´í¬ê°€ êº¼ì ¸ ìˆìœ¼ë©´ STT ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°
        if self.mic_enabled_callback and not self.mic_enabled_callback():
            return frame
        
        # WebRTC í”„ë ˆì„ ì •ë³´ ë¡œê¹… (ì²« í”„ë ˆì„ë§Œ ìƒì„¸)
        if not self._first_frame_logged:
            upstream_info = {}
            try:
                audio = frame.to_ndarray()
                upstream_info = {
                    'sample_rate': frame.sample_rate,
                    'format': str(frame.format) if hasattr(frame, 'format') else 'unknown',
                    'samples': frame.samples if hasattr(frame, 'samples') else 0,
                    'dtype': str(audio.dtype),
                    'shape': audio.shape
                }
            except:
                pass
            logger.info(f"ğŸ¤ First WebRTC frame: sr={upstream_info.get('sample_rate')}Hz, "
                      f"shape={upstream_info.get('shape')}, dtype={upstream_info.get('dtype')}")
            self._first_frame_logged = True
        
        # STTìš© 24kHz ë³€í™˜ ë° 20ms ì²­í¬ ìƒì„± (ë²„í¼ ê¸°ë°˜)
        stt_chunks, stt_metadata = self.audio_encoder.process_frame(frame)
        
        if not stt_chunks:
            return frame
        
        # ì˜¤ë””ì˜¤ ì—ë„ˆì§€ê°€ ë§¤ìš° ë‚®ìœ¼ë©´ ë§ˆì´í¬ê°€ êº¼ì§„ ê²ƒìœ¼ë¡œ ê°„ì£¼
        rms = stt_metadata.get('rms', 0.0)
        peak = stt_metadata.get('peak', 0)
        if rms < 0.001 and peak < 100:  # ë§¤ìš° ë‚®ì€ ì—ë„ˆì§€
            return frame
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self._update_stats(stt_metadata)
        
        # VADìš© 16kHz ì˜¤ë””ì˜¤ ìƒì„± (VAD íŒë‹¨ìš©)
        vad_bytes, vad_metadata = encode_audio_frame_for_vad(frame)
        is_speech = False
        if vad_bytes and len(vad_bytes) == 640:  # 16kHz, 20ms = 640 bytes
            try:
                is_speech = self.vad.is_speech(vad_bytes, 16000)
            except Exception as e:
                logger.debug(f"VAD detection error: {e}")
        
        # VADSegmenterë¡œ ì²˜ë¦¬ (ë§ ë ê°ì§€ ì‹œ ìë™ commit)
        if self.vad_segmenter and stt_chunks:
            for chunk_bytes in stt_chunks:
                # chunk_bytes ê²€ì¦ (960 bytes @ 24kHz)
                if len(chunk_bytes) != 960:
                    logger.error(f"âŒ Invalid chunk size: {len(chunk_bytes)} bytes (expected 960)")
                    continue
                
                # VADSegmenterì— ì „ë‹¬ (ë§ ë ê°ì§€ ì‹œ commit í˜¸ì¶œ)
                await self.vad_segmenter.process_chunk(chunk_bytes, is_speech, stt_metadata)
        
        return frame
    
    def _update_stats(self, metadata: dict):
        """í†µê³„ ì—…ë°ì´íŠ¸ (5ì´ˆë§ˆë‹¤ ë¡œê·¸ - ë¡œë´‡í†¤/ì •í™•ë„ ì²´í¬ë¦¬ìŠ¤íŠ¸)"""
        if not metadata:
            return
        
        self._stats['peak_sum'] += metadata.get('peak', 0)
        self._stats['rms_sum'] += metadata.get('rms', 0.0)
        self._stats['zero_ratio_sum'] += metadata.get('zero_ratio', 0.0)
        self._stats['clipped_ratio_sum'] += metadata.get('clipped_ratio', 0.0)
        self._stats['chunk_count'] += 1
        
        # 5ì´ˆë§ˆë‹¤ ìš”ì•½ ë¡œê·¸ (ë¡œë´‡í†¤/ì •í™•ë„ ì²´í¬ë¦¬ìŠ¤íŠ¸)
        current_time = time.time()
        if current_time - self._stats['last_log_time'] >= 5.0:
            if self._stats['chunk_count'] > 0:
                avg_peak = self._stats['peak_sum'] / self._stats['chunk_count']
                avg_rms = self._stats['rms_sum'] / self._stats['chunk_count']
                avg_zero_ratio = self._stats['zero_ratio_sum'] / self._stats['chunk_count']
                avg_clipped_ratio = self._stats['clipped_ratio_sum'] / self._stats['chunk_count']
                
                # ë¡œë´‡í†¤/ì •í™•ë„ ì²´í¬ë¦¬ìŠ¤íŠ¸ ë¡œê·¸
                upstream_info = metadata.get('upstream_info', {})
                upstream_shape = upstream_info.get('shape', 'unknown')
                resampled_samples = metadata.get('resampled_samples', 0)
                
                logger.info(f"ğŸ“Š Audio stats (5s): peak={avg_peak:.0f} (recommended: 3000~15000), "
                          f"rms={avg_rms:.4f}, zero_ratio={avg_zero_ratio:.2%}, "
                          f"clipped_ratio={avg_clipped_ratio:.2%}")
                logger.debug(f"   Upstream: shape={upstream_shape}, resampled_samples={resampled_samples}")
                
                # peak ê¶Œì¥ ë²”ìœ„ ì²´í¬
                if avg_peak < 3000:
                    logger.warning(f"âš ï¸ Low input level: peak={avg_peak:.0f} < 3000 (recommended: 3000~15000) â†’ STT accuracy may drop")
                elif avg_peak > 15000:
                    logger.warning(f"âš ï¸ High input level: peak={avg_peak:.0f} > 15000 (may cause clipping)")
            
            # í†µê³„ ë¦¬ì…‹
            self._stats = {
                'peak_sum': 0,
                'rms_sum': 0.0,
                'zero_ratio_sum': 0.0,
                'clipped_ratio_sum': 0.0,
                'chunk_count': 0,
                'last_log_time': current_time
            }
    

class AudioTrackSender(MediaStreamTrack):
    """TTS ì˜¤ë””ì˜¤ë¥¼ WebRTCë¡œ ì†¡ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""
    kind = "audio"
    
    def __init__(self):
        super().__init__()
        self._queue = asyncio.Queue()
        self._closed = False
        
    async def recv(self):
        """ì˜¤ë””ì˜¤ í”„ë ˆì„ ì†¡ì¶œ (20ms ë‹¨ìœ„)"""
        if self._closed:
            raise Exception("Track closed")
        
        # íì—ì„œ ì˜¤ë””ì˜¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        audio_data = await self._queue.get()
        return audio_data
    
    async def push_audio(self, audio_frame):
        """TTSì—ì„œ ìƒì„±ëœ ì˜¤ë””ì˜¤ë¥¼ íì— ì¶”ê°€"""
        if not self._closed:
            await self._queue.put(audio_frame)
    
    def close(self):
        """íŠ¸ë™ ì¢…ë£Œ"""
        self._closed = True


class WebRTCHandler:
    """WebRTC ì—°ê²° ë° ì˜¤ë””ì˜¤ ì²˜ë¦¬ í•¸ë“¤ëŸ¬"""
    
    def __init__(self, session_id: str, websocket: Optional[WebSocket] = None, enable_stt: bool = True):
        self.session_id = session_id
        self.websocket = websocket  # WebSocketì€ ì„ íƒì  (DataChannel ì‚¬ìš© ì‹œ None)
        self.enable_stt = enable_stt  # STT í™œì„±í™” ì—¬ë¶€
        self.pc: Optional[RTCPeerConnection] = None
        self.data_channel: Optional[RTCDataChannel] = None  # DataChannel
        
        # STT í´ë¼ì´ì–¸íŠ¸ (Realtime Transcription)
        self.stt_client: Optional[RealtimeSttClient] = None
        self.receiver_task: Optional[asyncio.Task] = None
        
        # ì„œë²„ VAD ëª¨ë“œ: VADSegmenter ì‚¬ìš© ì•ˆ í•¨
        
        # WAV ë¤í”„ (ë””ë²„ê¹…ìš©)
        self.debug_dump_wav = True  # ê°œë°œìš© í”Œë˜ê·¸
        self.stt_dump_seq = 0  # WAV ë¤í”„ ì‹œí€€ìŠ¤ ë²ˆí˜¸
        self.stt_accum_pcm16 = bytearray()  # WAV ë¤í”„ìš© ëˆ„ì  ë²„í¼
        
        # ì¹´ìš´í„° ê²€ì¦
        self.queued_chunks = 0  # íì— ë„£ì€ ì²­í¬ ìˆ˜
        self.sent_chunks = 0  # ì‹¤ì œë¡œ ì „ì†¡í•œ ì²­í¬ ìˆ˜
        self.appended_chunks = 0  # appendí•œ ì²­í¬ ìˆ˜
        
        # ì˜¤ë””ì˜¤ ì†¡ì¶œ íŠ¸ë™
        self.audio_sender: Optional[AudioTrackSender] = None
        
        # ìƒíƒœ ê´€ë¦¬
        self.is_speaking = False  # AIê°€ ë§í•˜ê³  ìˆëŠ”ì§€
        self.current_turn_cancelled = False  # Barge-in í”Œë˜ê·¸
        self.mic_enabled = True  # ë§ˆì´í¬ í™œì„±í™” ìƒíƒœ (ê¸°ë³¸ê°’: True)
        
    async def handle_connection(self):
        """WebRTC ì—°ê²° ì²˜ë¦¬"""
        self.pc = RTCPeerConnection()
        
        # ì˜¤ë””ì˜¤ ì†¡ì¶œ íŠ¸ë™ ìƒì„±
        self.audio_sender = AudioTrackSender()
        self.pc.addTrack(self.audio_sender)
        
        # ICE candidate ì²˜ë¦¬
        @self.pc.on("icecandidate")
        async def on_icecandidate(candidate):
            if candidate:
                # WebSocketì´ ìˆìœ¼ë©´ WebSocketìœ¼ë¡œ, ì—†ìœ¼ë©´ DataChannelë¡œ
                # í•˜ì§€ë§Œ ICE candidatesëŠ” ì—°ê²° ì „ì— ë°œìƒí•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë¬´ì‹œ
                # (ICE candidatesëŠ” ì´ë¯¸ SDPì— í¬í•¨ë˜ì–´ ìˆìŒ)
                pass
        
        # ì—°ê²° ìƒíƒœ ë³€ê²½
        @self.pc.on("connectionstatechange")
        async def on_connectionstatechange():
            logger.info(f"Connection state: {self.pc.connectionState}")
            if self.pc.connectionState == "failed":
                await self.cleanup()
        
        # DataChannel ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì„¤ì •
        self._setup_datachannel_handlers()
        
        # ì˜¤ë””ì˜¤ íŠ¸ë™ ìˆ˜ì‹  ì²˜ë¦¬
        @self.pc.on("track")
        async def on_track(track):
            if track.kind == "audio":
                logger.info("Audio track received")
                # STT í´ë¼ì´ì–¸íŠ¸ê°€ ìˆìœ¼ë©´ AudioTrackReceiver ìƒì„± (ì„œë²„ VAD ëª¨ë“œ)
                if self.stt_client:
                    receiver = AudioTrackReceiver(
                        track, 
                        self.stt_client,
                        mic_enabled_callback=lambda: self.mic_enabled,
                        digital_gain_db=6.0  # ì…ë ¥ ê²Œì¸ 6dB (peak 3000~15000 ë²”ìœ„ë¡œ ì¡°ì •)
                    )
                    asyncio.create_task(self._audio_receive_loop(receiver))
                else:
                    logger.warning("STT client not initialized, audio processing skipped")
        
        # WebSocketì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì‹œê·¸ë„ë§ ë©”ì‹œì§€ ì²˜ë¦¬ (ì´ì „ ë°©ì‹)
        if self.websocket:
            while True:
                try:
                    message = await self.websocket.receive_json()
                    await self._handle_signaling(message)
                except Exception as e:
                    logger.error(f"Signaling error: {e}", exc_info=True)
                    break
    
    async def _audio_receive_loop(self, receiver: AudioTrackReceiver):
        """ì˜¤ë””ì˜¤ ìˆ˜ì‹  ë£¨í”„"""
        frame_count = 0
        last_log_time = 0
        try:
            while True:
                frame = await receiver.recv()
                frame_count += 1
                
                # 1ì´ˆë§ˆë‹¤ í”„ë ˆì„ ìˆ˜ì‹  ìƒíƒœ ë¡œê·¸ (ë””ë²„ê¹…ìš©)
                import time
                current_time = time.time()
                if current_time - last_log_time >= 1.0:
            
                    last_log_time = current_time
                
                # recv() ë‚´ë¶€ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë¨
        except Exception as e:
            logger.error(f"Audio receive loop error: {e}")
            logger.info(f"ğŸ“Š ì˜¤ë””ì˜¤ ìˆ˜ì‹  ë£¨í”„ ì¢…ë£Œ (ì´ {frame_count}ê°œ í”„ë ˆì„ ìˆ˜ì‹ ë¨)")
    
    async def _handle_ice_candidate(self, msg: dict):
        """
        ICE candidate ì²˜ë¦¬
        msg can be:
          {type:'ice-candidate', candidate:'candidate:...', sdpMid:'0', sdpMLineIndex:0}
        or
          {type:'ice-candidate', candidate:{candidate:'candidate:...', sdpMid:'0', sdpMLineIndex:0}}
        """
        c = msg.get("candidate")
        if not c:
            logger.debug("ICE candidate message missing candidate field")
            return

        # normalize nested format
        if isinstance(c, dict):
            candidate_sdp = c.get("candidate")
            sdp_mid = c.get("sdpMid")
            sdp_mline_index = c.get("sdpMLineIndex")
        else:
            candidate_sdp = c
            sdp_mid = msg.get("sdpMid")
            sdp_mline_index = msg.get("sdpMLineIndex")

        if not candidate_sdp:
            logger.warning("ICE candidate message missing candidate SDP string")
            return

        # PeerConnectionì´ ìœ íš¨í•œì§€ í™•ì¸
        if not self.pc or self.pc.connectionState == "closed":
            logger.debug("PeerConnection not available or closed, skipping ICE candidate")
            return

        try:
            # candidate_from_sdpë¡œ íŒŒì‹±
            cand = candidate_from_sdp(candidate_sdp)
            if sdp_mid is not None:
                cand.sdpMid = sdp_mid
            if sdp_mline_index is not None:
                cand.sdpMLineIndex = sdp_mline_index
            
            await self.pc.addIceCandidate(cand)
            logger.info("ICE candidate added (mid=%s, mline=%s)", sdp_mid, sdp_mline_index)
        except Exception as e:
            # candidate ì²˜ë¦¬ ì‹¤íŒ¨í•´ë„ ì„¸ì…˜ì„ ëŠì§€ ì•ŠìŒ
            logger.warning("Failed to add ICE candidate: %s | msg=%s", e, msg)
    
    async def handle_offer(self, sdp_offer: str) -> str:
        """
        HTTP POSTë¡œ ë°›ì€ offerë¥¼ ì²˜ë¦¬í•˜ê³  answerë¥¼ ë°˜í™˜
        """
        # PeerConnection ìƒì„±
        self.pc = RTCPeerConnection()
        
        # ì˜¤ë””ì˜¤ ì†¡ì¶œ íŠ¸ë™ ìƒì„±
        self.audio_sender = AudioTrackSender()
        self.pc.addTrack(self.audio_sender)
        
        # DataChannel ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì„¤ì •
        self._setup_datachannel_handlers()
        
        # ì—°ê²° ìƒíƒœ ë³€ê²½
        @self.pc.on("connectionstatechange")
        async def on_connectionstatechange():
            logger.info(f"Connection state: {self.pc.connectionState}")
            if self.pc.connectionState == "failed":
                await self.cleanup()
        
        # STT íŒŒì´í”„ë¼ì¸ ë¨¼ì € ì´ˆê¸°í™” (ì˜¤ë””ì˜¤ íŠ¸ë™ í•¸ë“¤ëŸ¬ ë“±ë¡ ì „ì—)
        if self.enable_stt:
            try:
                await self._setup_stt_pipeline()
            except Exception as e:
                logger.error(f"Failed to setup STT during handle_offer: {e}", exc_info=True)
                # STT ì‹¤íŒ¨í•´ë„ WebRTC ì—°ê²°ì€ ê³„ì† ì§„í–‰
        else:
            logger.info("STT is disabled for this session - WebRTC call only")
        
        # ì˜¤ë””ì˜¤ íŠ¸ë™ ìˆ˜ì‹  ì²˜ë¦¬ (STT íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” í›„)
        @self.pc.on("track")
        async def on_track(track):
            if track.kind == "audio":
                logger.info("âœ… Audio track received from client")
                # STT í´ë¼ì´ì–¸íŠ¸ê°€ ìˆìœ¼ë©´ AudioTrackReceiver ìƒì„± (ì„œë²„ VAD ëª¨ë“œ)
                if self.stt_client:
                    receiver = AudioTrackReceiver(
                        track, 
                        self.stt_client,
                        mic_enabled_callback=lambda: self.mic_enabled,
                        digital_gain_db=6.0  # ì…ë ¥ ê²Œì¸ 6dB (peak 3000~15000 ë²”ìœ„ë¡œ ì¡°ì •)
                    )
                    asyncio.create_task(self._audio_receive_loop(receiver))
                else:
                    logger.warning("STT client not initialized, audio processing skipped")
        
        # offer ì„¤ì •
        offer = RTCSessionDescription(sdp=sdp_offer, type="offer")
        await self.pc.setRemoteDescription(offer)
        
        # answer ìƒì„±
        answer = await self.pc.createAnswer()
        await self.pc.setLocalDescription(answer)
        
        logger.info("âœ… WebRTC answer ìƒì„± ì™„ë£Œ")
        
        return self.pc.localDescription.sdp
    
    async def _wait_for_connection(self):
        """ì—°ê²° ì™„ë£Œ ëŒ€ê¸° (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)"""
        try:
            # ICE ì—°ê²° ì™„ë£Œ ëŒ€ê¸° (ìµœëŒ€ 10ì´ˆ)
            for _ in range(100):  # 100 * 0.1ì´ˆ = 10ì´ˆ
                if self.pc and self.pc.connectionState == "connected":
                    logger.info("âœ… WebRTC connection established")
                    break
                await asyncio.sleep(0.1)
        except Exception as e:
            logger.error(f"Error waiting for connection: {e}")
    
    def _setup_datachannel_handlers(self):
        """DataChannel ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì„¤ì •"""
        @self.pc.on("datachannel")
        def on_datachannel(channel: RTCDataChannel):
            logger.info(f"DataChannel received: {channel.label}")
            self.data_channel = channel
            
            @channel.on("message")
            def on_message(message):
                try:
                    if isinstance(message, str):
                        data = json.loads(message)
                        logger.debug(f"DataChannel message received: {data}")
                        # ë§ˆì´í¬ ìƒíƒœ ë©”ì‹œì§€ ì²˜ë¦¬
                        asyncio.create_task(self._handle_datachannel_message(data))
                except Exception as e:
                    logger.error(f"Error processing DataChannel message: {e}")
            
            @channel.on("open")
            def on_open():
                logger.info("âœ… DataChannel opened")
            
            @channel.on("close")
            def on_close():
                logger.info("DataChannel closed")
    
    async def _handle_datachannel_message(self, data: dict):
        """DataChannel ë©”ì‹œì§€ ì²˜ë¦¬"""
        msg_type = data.get("type")
        
        if msg_type == "mic.enabled" or msg_type == "mic.on":
            self.mic_enabled = True
            logger.info("ğŸ¤ Microphone enabled")
        elif msg_type == "mic.disabled" or msg_type == "mic.off":
            self.mic_enabled = False
            logger.info("ğŸ”‡ Microphone disabled")
            # ë§ˆì´í¬ê°€ êº¼ì§€ë©´ STT í´ë¼ì´ì–¸íŠ¸ ë²„í¼ ì •ë¦¬ (server_vad ëª¨ë“œì—ì„œëŠ” clear ì‚¬ìš© ì•ˆ í•¨)
            # server_vad ëª¨ë“œì—ì„œëŠ” ì„œë²„ê°€ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ ë³„ë„ ì‘ì—… ë¶ˆí•„ìš”
        elif msg_type == "mic.toggle":
            self.mic_enabled = not self.mic_enabled
            logger.info(f"ğŸ¤ Microphone toggled: {'ON' if self.mic_enabled else 'OFF'}")
        # ê¸°íƒ€ ë©”ì‹œì§€ëŠ” ë¬´ì‹œ
    
    async def _send_datachannel_message(self, message: dict):
        """DataChannelë¡œ ë©”ì‹œì§€ ì „ì†¡"""
        if self.data_channel and self.data_channel.readyState == "open":
            try:
                message_str = json.dumps(message)
                self.data_channel.send(message_str)
                logger.debug(f"ğŸ“¤ DataChannel sent: {message.get('type', 'unknown')}")
            except Exception as e:
                logger.error(f"âŒ Error sending DataChannel message: {e}", exc_info=True)
        else:
            logger.warning(f"âš ï¸ DataChannel is not open (state: {self.data_channel.readyState if self.data_channel else 'None'}), cannot send message: {message.get('type', 'unknown')}")
    
    async def _handle_signaling(self, message: dict):
        """WebRTC ì‹œê·¸ë„ë§ ë©”ì‹œì§€ ì²˜ë¦¬ (WebSocket ë°©ì‹ìš©)"""
        msg_type = message.get("type")
        
        if msg_type == "offer":
            # í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° offer ë°›ìŒ
            offer = RTCSessionDescription(
                sdp=message["sdp"],
                type="offer"
            )
            await self.pc.setRemoteDescription(offer)
            
            # answer ìƒì„±
            answer = await self.pc.createAnswer()
            await self.pc.setLocalDescription(answer)
            
            # answer ì „ì†¡ (WebSocket)
            if self.websocket:
                await self.websocket.send_json({
                    "type": "answer",
                    "sdp": self.pc.localDescription.sdp
                })
            
        elif msg_type == "ice-candidate":
            # ICE candidate ì²˜ë¦¬
            await self._handle_ice_candidate(message)
    
    async def _setup_stt_pipeline(self):
        """STT íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (ë¡œì»¬ VAD ëª¨ë“œ: VADë¡œ ë§ ë ê°ì§€ í›„ commit)"""
        if not self.enable_stt:
            return
        
        logger.info(f"[STT Setup] Starting STT pipeline setup for session: {self.session_id} (Local VAD mode)")
        
        try:
            # STT í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë° ì—°ê²°
            self.stt_client = RealtimeSttClient(self.session_id)
            await self.stt_client.connect()
            logger.info("âœ… [STT Setup] STT client connected (Local VAD mode)")
            
            # ë¡œì»¬ VAD ëª¨ë“œ: VADSegmenterê°€ ë§ ëì„ ê°ì§€í•˜ë©´ commit í˜¸ì¶œ
            
            # Receiver ì›Œì»¤ ì‹œì‘
            self.receiver_task = asyncio.create_task(
                self._stt_receiver_worker()
            )
            logger.info("âœ… [STT Setup] STT receiver worker started")
            
        except Exception as e:
            logger.error(f"âŒ [STT Setup] Failed to setup STT pipeline: {e}", exc_info=True)
            self.stt_client = None
    
    # ì„œë²„ VAD ëª¨ë“œ: clear/commit ì½œë°± ì œê±° (appendë§Œ ì—°ì† ì „ì†¡)
    
    async def _dump_wav_file(self):
        """WAV ë¤í”„ ì €ì¥ (OpenAIë¡œ ë³´ë‚´ëŠ” ìµœì¢… 24kHz PCM16)"""
        try:
            if len(self.stt_accum_pcm16) == 0:
                return
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            dump_dir = "stt_dumps"
            os.makedirs(dump_dir, exist_ok=True)
            
            # íŒŒì¼ëª… ìƒì„±
            self.stt_dump_seq += 1
            stt_stats = self.stt_client.get_stats() if self.stt_client else {}
            sample_rate = stt_stats.get('sample_rate', 24000)  # 24kHz ê¸°ë³¸
            duration_ms = self.appended_chunks * 20  # 20ms per chunk
            filename = f"{dump_dir}/stt_session_{self.session_id}_{self.stt_dump_seq:04d}_{duration_ms}ms_{sample_rate//1000}k.wav"
            
            # WAV íŒŒì¼ ì €ì¥ (24kHz ê¸°ì¤€) - ì¬ìƒ ì‹œ 24kHzë¡œ ì„¤ì •í•´ì•¼ ì •ìƒ ìŒì„±ì²˜ëŸ¼ ë“¤ë¦¼
            with wave.open(filename, 'wb') as wav_file:
                wav_file.setnchannels(1)  # mono
                wav_file.setsampwidth(2)  # 16-bit = 2 bytes
                wav_file.setframerate(sample_rate)  # 24kHz (ì¤‘ìš”: ì¬ìƒ ì‹œ ê°™ì€ rateë¡œ ì„¤ì •)
                wav_file.writeframes(bytes(self.stt_accum_pcm16))
            
            if os.path.exists(filename):
                file_size = os.path.getsize(filename)
                logger.debug(f"WAV dumped: {filename} ({duration_ms}ms, {file_size} bytes, {sample_rate}Hz)")
        
        except Exception as e:
            logger.error(f"STT: Error dumping WAV: {e}", exc_info=True)
    
    async def _stt_receiver_worker(self):
        """STT ê²°ê³¼ ìˆ˜ì‹  ì›Œì»¤ (partial/final ì „ì‚¬ ê²°ê³¼ ì²˜ë¦¬)"""
        if not self.stt_client:
            return
        
        # STT í†µê³„ ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ ì‹œì‘ (10ì´ˆë§ˆë‹¤ í™•ì¸)
        async def monitor_stt_stats():
            while True:
                await asyncio.sleep(10.0)
                if self.stt_client:
                    stats = self.stt_client.get_stats()
                    appended_chunks = stats.get('appended_chunks', 0)
                    buffered_ms = stats.get('buffered_ms', 0)
                    if appended_chunks == 0:
                        logger.warning(f"âš ï¸ STT stats check: appended_chunks=0 (no audio sent to STT!)")
                    else:
                        logger.info(f"ğŸ“Š STT stats: {appended_chunks} chunks appended, {buffered_ms}ms buffered")
        
        monitor_task = asyncio.create_task(monitor_stt_stats())
        
        try:
            await self.stt_client.start_receiver_loop(
                on_partial=self._on_stt_partial,
                on_final=self._on_stt_final,
                on_error=self._on_stt_error
            )
        except asyncio.CancelledError:
            logger.debug("STT receiver worker cancelled")
            monitor_task.cancel()
        except Exception as e:
            logger.error(f"STT receiver worker error: {e}", exc_info=True)
            monitor_task.cancel()
    
    async def _on_stt_partial(self, text: str):
        """STT partial ê²°ê³¼ ì²˜ë¦¬"""
        if not text or not text.strip():
            return
        
        text_clean = text.strip()
        logger.info(f"ğŸ“ STT partial: {text_clean}")
        
        await self._send_datachannel_message({
            "type": "stt.partial",
            "text": text_clean
        })
    
    async def _on_stt_final(self, text: str):
        """STT final ê²°ê³¼ ì²˜ë¦¬"""
        if not text or not text.strip():
            return
        
        text_clean = text.strip()
        logger.info(f"âœ… STT final: {text_clean}")
        
        await self._send_datachannel_message({
            "type": "stt.final",
            "text": text_clean
        })
    
    async def _on_stt_error(self, error: Exception):
        """STT ì—ëŸ¬ ì²˜ë¦¬"""
        logger.error(f"STT error: {error}", exc_info=True)
        await self._send_datachannel_message({
            "type": "stt.error",
            "message": str(error)
        })
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info(f"Cleaning up session: {self.session_id}")
        
        # ì„œë²„ VAD ëª¨ë“œ: VADSegmenter ì‚¬ìš© ì•ˆ í•¨
        
        # STT ì›Œì»¤ ì •ë¦¬
        if self.receiver_task:
            self.receiver_task.cancel()
            try:
                await self.receiver_task
            except asyncio.CancelledError:
                pass
            self.receiver_task = None
        
        # STT í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ
        if self.stt_client:
            try:
                await self.stt_client.close()
            except Exception as e:
                logger.warning(f"Error closing STT client: {e}")
            self.stt_client = None
        
        # WebRTC ì—°ê²° ì¢…ë£Œ
        if self.pc:
            try:
                # ì—°ê²° ìƒíƒœ í™•ì¸ í›„ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ
                if self.pc.connectionState != "closed":
                    await self.pc.close()
            except Exception as e:
                logger.warning(f"Error closing PeerConnection: {e}")
            finally:
                self.pc = None
        
        # ì˜¤ë””ì˜¤ ì†¡ì¶œ íŠ¸ë™ ì¢…ë£Œ
        if self.audio_sender:
            try:
                self.audio_sender.close()
            except Exception as e:
                logger.warning(f"Error closing audio sender: {e}")
            finally:
                self.audio_sender = None
