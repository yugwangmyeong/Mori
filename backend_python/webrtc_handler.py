"""
WebRTC ì—°ê²° ë° ì˜¤ë””ì˜¤ ì²˜ë¦¬ í•¸ë“¤ëŸ¬
"""
import asyncio
import logging
import numpy as np
from typing import Optional, List, Callable
from collections import deque
from fastapi import WebSocket
from aiortc import RTCPeerConnection, RTCSessionDescription, RTCIceCandidate, MediaStreamTrack, RTCDataChannel
from aiortc.contrib.media import MediaPlayer, MediaRelay
from aiortc.sdp import candidate_from_sdp
import json

from audio_processor import AudioProcessor
# TODO: ë‹¤ìŒ ë‹¨ê³„ì—ì„œ í™œì„±í™”
# from vad_processor import VADProcessor
# from stt_service import STTService
# from llm_service import LLMService
# from tts_service import TTSService

logger = logging.getLogger(__name__)


class AudioTrackReceiver(MediaStreamTrack):
    """WebRTCì—ì„œ ë°›ì€ ì˜¤ë””ì˜¤ íŠ¸ë™ì„ ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ (VAD ê¸°ë°˜ ìƒíƒœë¨¸ì‹ )"""
    kind = "audio"
    
    # ìƒíƒœë¨¸ì‹  ìƒíƒœ
    STATE_IDLE = "IDLE"  # ë§ ì•ˆ í•¨
    STATE_IN_SPEECH = "IN_SPEECH"  # ë§ ì¤‘
    
    def __init__(self, track, audio_processor: AudioProcessor, on_speech_end=None):
        super().__init__()
        self.track = track
        self.audio_processor = audio_processor
        self.on_speech_end = on_speech_end  # ë§ ë ì½œë°± (audio_bytes: bytes) -> None
        
        # VAD í”„ë ˆì„ ì •ê·œí™” íŒŒë¼ë¯¸í„°
        self.VAD_SR = 16000  # VAD ìƒ˜í”Œë ˆì´íŠ¸ (webrtcvadëŠ” 16kHzë§Œ ì§€ì›)
        self.VAD_FRAME_MS = 20  # VAD í”„ë ˆì„ ê¸¸ì´ (10/20/30msë§Œ ì§€ì›)
        self.VAD_BYTES = int(self.VAD_SR * (self.VAD_FRAME_MS / 1000.0) * 2)  # 640 bytes (16kHz * 20ms * 2 bytes/sample)
        self._vad_buf = bytearray()  # VAD í”„ë ˆì„ ë²„í¼ (20ms ë‹¨ìœ„ë¡œ ìª¼ê°œê¸° ìœ„í•´)
        
        # ìƒíƒœë¨¸ì‹  ìƒíƒœ
        self._vad_state = self.STATE_IDLE
        self._speech_buf = bytearray()  # speech êµ¬ê°„ ì˜¤ë””ì˜¤ ë²„í¼ (PCM16 bytes)
        
        # VAD ìœˆë„ìš° (ìµœê·¼ 400ms, 20í”„ë ˆì„ * 20ms = 400ms)
        self._vad_window = deque(maxlen=20)  # ê° í”„ë ˆì„ì˜ speech ì—¬ë¶€ (1=speech, 0=silence)
        
    async def recv(self):
        """ì˜¤ë””ì˜¤ í”„ë ˆì„ ìˆ˜ì‹  - VAD ê¸°ë°˜ ìƒíƒœë¨¸ì‹  (20ms í”„ë ˆì„ ì •ê·œí™”)"""
        frame = await self.track.recv()
        
        # ê²€ì¦ ë¡œê·¸ (ì¼ì‹œì , ë””ë²„ê¹…ìš©)
        try:
            x = frame.to_ndarray()
            logger.debug(f"[FRAME] dtype={x.dtype}, shape={x.shape}, sr={frame.sample_rate}")
        except:
            pass
        
        # VAD íŒë‹¨ìš© PCM16 ë³€í™˜ (ê°€ë³ê²Œ, í•­ìƒ 640 bytes ë°˜í™˜)
        pcm16_bytes = self.audio_processor.to_pcm16_16k_mono(frame)
        
        if pcm16_bytes is None:
            # ë³€í™˜ ì‹¤íŒ¨ ì‹œ í”„ë ˆì„ë§Œ ë°˜í™˜ (ì²˜ë¦¬ ì—†ìŒ)
            return frame
        
        # VAD í”„ë ˆì„ ë²„í¼ì— ì¶”ê°€
        self._vad_buf.extend(pcm16_bytes)
        
        # 20ms í”„ë ˆì„(640 bytes) ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ VAD ì²˜ë¦¬
        while len(self._vad_buf) >= self.VAD_BYTES:
            # 20ms í”„ë ˆì„ ì¶”ì¶œ
            vad_frame = bytes(self._vad_buf[:self.VAD_BYTES])
            del self._vad_buf[:self.VAD_BYTES]
            
            # VAD ë””ë²„ê·¸ ë¡œê·¸ (ê²€ì¦ìš© - ì¼ì‹œì )
            logger.debug(f"[VAD FRAME] sr={self.VAD_SR}, bytes={len(vad_frame)}, samples={len(vad_frame)//2}")
            
            # VADë¡œ speech ì—¬ë¶€ íŒë‹¨ (ë°˜ë“œì‹œ 16000 ìƒ˜í”Œë ˆì´íŠ¸ ì „ë‹¬, 640 bytes)
            try:
                is_speech = self.audio_processor.vad.is_speech(vad_frame, self.VAD_SR)
            except Exception as e:
                logger.error(f"VAD error: {e}", exc_info=True)
                # ì—ëŸ¬ ë°œìƒ ì‹œ ì´ í”„ë ˆì„ì€ ìŠ¤í‚µí•˜ê³  ë‹¤ìŒ í”„ë ˆì„ ì²˜ë¦¬
                break
            
            # VAD ìœˆë„ìš°ì— ì¶”ê°€ (1=speech, 0=silence)
            self._vad_window.append(1 if is_speech else 0)
            
            # ìƒíƒœë¨¸ì‹  ì²˜ë¦¬
            if len(self._vad_window) < self._vad_window.maxlen:
                # ìœˆë„ìš°ê°€ ì±„ì›Œì§€ì§€ ì•Šì•˜ìœ¼ë©´ ìƒíƒœ ì „í™˜í•˜ì§€ ì•ŠìŒ
                continue
            
            # speech ratio ê³„ì‚°
            ratio = sum(self._vad_window) / len(self._vad_window)
            
            if self._vad_state == self.STATE_IDLE:
                # IDLE ìƒíƒœ: ë§ ì‹œì‘ ì¡°ê±´ í™•ì¸
                if ratio >= 0.4:  # 40% ì´ìƒ speechë©´ ë§ ì‹œì‘
                    self._vad_state = self.STATE_IN_SPEECH
                    logger.info("ğŸ™ï¸ speech_start")
                    self._speech_buf.extend(vad_frame)  # ì‹œì‘ í”„ë ˆì„ í¬í•¨
                else:
                    # ë¬´ìŒì´ë©´ ì—¬ê¸°ì„œ ë - ë²„í¼/ì¸ì½”ë”©/ë¡œê·¸/STT ì—†ìŒ
                    continue
                
            elif self._vad_state == self.STATE_IN_SPEECH:
                # IN_SPEECH ìƒíƒœ: ì˜¤ë””ì˜¤ ë²„í¼ì— ì¶”ê°€
                self._speech_buf.extend(vad_frame)
                
                # ë§ ë ì¡°ê±´: 400ms ë™ì•ˆ ê±°ì˜ ë¬´ìŒ (10% ì´í•˜)
                if ratio <= 0.1:
                    logger.info("ğŸ›‘ speech_end")
                    
                    # speech êµ¬ê°„ ì˜¤ë””ì˜¤ í™•ì •
                    audio_for_stt = bytes(self._speech_buf)
                    self._speech_buf.clear()
                    self._vad_state = self.STATE_IDLE
                    self._vad_window.clear()
                    
                    # speech_endì—ì„œë§Œ "ì¸ì½”ë”© ì™„ë£Œ / STT ì „ì†¡" ë¡œê·¸ ì¶œë ¥
                    logger.info(f"âœ… speech segment bytes={len(audio_for_stt)} (16kHz mono PCM16)")
                    
                    # TODO: ì—¬ê¸°ì„œë§Œ STT í˜¸ì¶œ or í enqueue
                    if self.on_speech_end:
                        try:
                            # bytesë¥¼ numpy arrayë¡œ ë³€í™˜í•˜ì—¬ ì½œë°±ì— ì „ë‹¬
                            audio_array = np.frombuffer(audio_for_stt, dtype=np.int16)
                            await self.on_speech_end(audio_array)
                        except Exception as e:
                            logger.error(f"Error in on_speech_end callback: {e}", exc_info=True)
        
        return frame
    


class AudioTrackSender(MediaStreamTrack):
    """TTS ì˜¤ë””ì˜¤ë¥¼ WebRTCë¡œ ì†¡ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""
    kind = "audio"
    
    def __init__(self):
        super().__init__()
        self._queue = asyncio.Queue()
        self._closed = False
        
    async def recv(self):
        """ì˜¤ë””ì˜¤ í”„ë ˆì„ ì†¡ì¶œ (20ms ë‹¨ìœ„)"""
        if self._closed:
            raise Exception("Track closed")
        
        # íì—ì„œ ì˜¤ë””ì˜¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        audio_data = await self._queue.get()
        return audio_data
    
    async def push_audio(self, audio_frame):
        """TTSì—ì„œ ìƒì„±ëœ ì˜¤ë””ì˜¤ë¥¼ íì— ì¶”ê°€"""
        if not self._closed:
            await self._queue.put(audio_frame)
    
    def close(self):
        """íŠ¸ë™ ì¢…ë£Œ"""
        self._closed = True


class WebRTCHandler:
    """WebRTC ì—°ê²° ë° ì˜¤ë””ì˜¤ ì²˜ë¦¬ í•¸ë“¤ëŸ¬"""
    
    def __init__(self, session_id: str, websocket: Optional[WebSocket] = None):
        self.session_id = session_id
        self.websocket = websocket  # WebSocketì€ ì„ íƒì  (DataChannel ì‚¬ìš© ì‹œ None)
        self.pc: Optional[RTCPeerConnection] = None
        self.data_channel: Optional[RTCDataChannel] = None  # DataChannel
        
        # ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì»´í¬ë„ŒíŠ¸
        self.audio_processor = AudioProcessor()
        # TODO: ë‹¤ìŒ ë‹¨ê³„ì—ì„œ í™œì„±í™”
        # self.vad_processor = VADProcessor(
        #     on_speech_end=self._on_speech_end,
        #     on_speech_start=self._on_speech_start
        # )
        # self.stt_service = STTService()
        # self.llm_service = LLMService()
        # self.tts_service = TTSService()
        
        # ì˜¤ë””ì˜¤ ì†¡ì¶œ íŠ¸ë™
        self.audio_sender: Optional[AudioTrackSender] = None
        
        # ìƒíƒœ ê´€ë¦¬
        self.is_speaking = False  # AIê°€ ë§í•˜ê³  ìˆëŠ”ì§€
        self.current_turn_cancelled = False  # Barge-in í”Œë˜ê·¸
        
    async def handle_connection(self):
        """WebRTC ì—°ê²° ì²˜ë¦¬"""
        self.pc = RTCPeerConnection()
        
        # ì˜¤ë””ì˜¤ ì†¡ì¶œ íŠ¸ë™ ìƒì„±
        self.audio_sender = AudioTrackSender()
        self.pc.addTrack(self.audio_sender)
        
        # ICE candidate ì²˜ë¦¬
        @self.pc.on("icecandidate")
        async def on_icecandidate(candidate):
            if candidate:
                # WebSocketì´ ìˆìœ¼ë©´ WebSocketìœ¼ë¡œ, ì—†ìœ¼ë©´ DataChannelë¡œ
                # í•˜ì§€ë§Œ ICE candidatesëŠ” ì—°ê²° ì „ì— ë°œìƒí•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë¬´ì‹œ
                # (ICE candidatesëŠ” ì´ë¯¸ SDPì— í¬í•¨ë˜ì–´ ìˆìŒ)
                pass
        
        # ì—°ê²° ìƒíƒœ ë³€ê²½
        @self.pc.on("connectionstatechange")
        async def on_connectionstatechange():
            logger.info(f"Connection state: {self.pc.connectionState}")
            if self.pc.connectionState == "failed":
                await self.cleanup()
        
        # DataChannel ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì„¤ì •
        self._setup_datachannel_handlers()
        
        # ì˜¤ë””ì˜¤ íŠ¸ë™ ìˆ˜ì‹  ì²˜ë¦¬
        @self.pc.on("track")
        async def on_track(track):
            if track.kind == "audio":
                logger.info("Audio track received")
                # VAD ê¸°ë°˜ ìƒíƒœë¨¸ì‹ ìœ¼ë¡œ ì˜¤ë””ì˜¤ ìˆ˜ì‹ 
                receiver = AudioTrackReceiver(
                    track, 
                    self.audio_processor, 
                    on_speech_end=self._on_speech_end
                )
                # íŠ¸ë™ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ ë£¨í”„ ì‹¤í–‰
                asyncio.create_task(self._audio_receive_loop(receiver))
        
        # WebSocketì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì‹œê·¸ë„ë§ ë©”ì‹œì§€ ì²˜ë¦¬ (ì´ì „ ë°©ì‹)
        if self.websocket:
            while True:
                try:
                    message = await self.websocket.receive_json()
                    await self._handle_signaling(message)
                except Exception as e:
                    logger.error(f"Signaling error: {e}", exc_info=True)
                    break
    
    async def _audio_receive_loop(self, receiver: AudioTrackReceiver):
        """ì˜¤ë””ì˜¤ ìˆ˜ì‹  ë£¨í”„"""
        frame_count = 0
        last_log_time = 0
        try:
            while True:
                frame = await receiver.recv()
                frame_count += 1
                
                # 1ì´ˆë§ˆë‹¤ í”„ë ˆì„ ìˆ˜ì‹  ìƒíƒœ ë¡œê·¸ (ë””ë²„ê¹…ìš©)
                import time
                current_time = time.time()
                if current_time - last_log_time >= 1.0:
                    logger.info(f"ğŸ“Š ì˜¤ë””ì˜¤ í”„ë ˆì„ ìˆ˜ì‹  ì¤‘... (ì´ {frame_count}ê°œ í”„ë ˆì„ ìˆ˜ì‹ ë¨)")
                    last_log_time = current_time
                
                # recv() ë‚´ë¶€ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë¨
        except Exception as e:
            logger.error(f"Audio receive loop error: {e}")
            logger.info(f"ğŸ“Š ì˜¤ë””ì˜¤ ìˆ˜ì‹  ë£¨í”„ ì¢…ë£Œ (ì´ {frame_count}ê°œ í”„ë ˆì„ ìˆ˜ì‹ ë¨)")
    
    async def _handle_ice_candidate(self, msg: dict):
        """
        ICE candidate ì²˜ë¦¬
        msg can be:
          {type:'ice-candidate', candidate:'candidate:...', sdpMid:'0', sdpMLineIndex:0}
        or
          {type:'ice-candidate', candidate:{candidate:'candidate:...', sdpMid:'0', sdpMLineIndex:0}}
        """
        c = msg.get("candidate")
        if not c:
            logger.debug("ICE candidate message missing candidate field")
            return

        # normalize nested format
        if isinstance(c, dict):
            candidate_sdp = c.get("candidate")
            sdp_mid = c.get("sdpMid")
            sdp_mline_index = c.get("sdpMLineIndex")
        else:
            candidate_sdp = c
            sdp_mid = msg.get("sdpMid")
            sdp_mline_index = msg.get("sdpMLineIndex")

        if not candidate_sdp:
            logger.warning("ICE candidate message missing candidate SDP string")
            return

        # PeerConnectionì´ ìœ íš¨í•œì§€ í™•ì¸
        if not self.pc or self.pc.connectionState == "closed":
            logger.debug("PeerConnection not available or closed, skipping ICE candidate")
            return

        try:
            # candidate_from_sdpë¡œ íŒŒì‹±
            cand = candidate_from_sdp(candidate_sdp)
            if sdp_mid is not None:
                cand.sdpMid = sdp_mid
            if sdp_mline_index is not None:
                cand.sdpMLineIndex = sdp_mline_index
            
            await self.pc.addIceCandidate(cand)
            logger.info("ICE candidate added (mid=%s, mline=%s)", sdp_mid, sdp_mline_index)
        except Exception as e:
            # candidate ì²˜ë¦¬ ì‹¤íŒ¨í•´ë„ ì„¸ì…˜ì„ ëŠì§€ ì•ŠìŒ
            logger.warning("Failed to add ICE candidate: %s | msg=%s", e, msg)
    
    async def handle_offer(self, sdp_offer: str) -> str:
        """
        HTTP POSTë¡œ ë°›ì€ offerë¥¼ ì²˜ë¦¬í•˜ê³  answerë¥¼ ë°˜í™˜
        """
        # PeerConnection ìƒì„±
        self.pc = RTCPeerConnection()
        
        # ì˜¤ë””ì˜¤ ì†¡ì¶œ íŠ¸ë™ ìƒì„±
        self.audio_sender = AudioTrackSender()
        self.pc.addTrack(self.audio_sender)
        
        # DataChannel ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì„¤ì •
        self._setup_datachannel_handlers()
        
        # ì˜¤ë””ì˜¤ íŠ¸ë™ ìˆ˜ì‹  ì²˜ë¦¬
        @self.pc.on("track")
        async def on_track(track):
            if track.kind == "audio":
                logger.info("âœ… Audio track received from client")
                # VAD ê¸°ë°˜ ìƒíƒœë¨¸ì‹ ìœ¼ë¡œ ì˜¤ë””ì˜¤ ìˆ˜ì‹ 
                receiver = AudioTrackReceiver(
                    track, 
                    self.audio_processor, 
                    on_speech_end=self._on_speech_end
                )
                asyncio.create_task(self._audio_receive_loop(receiver))
        
        # ì—°ê²° ìƒíƒœ ë³€ê²½
        @self.pc.on("connectionstatechange")
        async def on_connectionstatechange():
            logger.info(f"Connection state: {self.pc.connectionState}")
            if self.pc.connectionState == "failed":
                await self.cleanup()
        
        # offer ì„¤ì •
        offer = RTCSessionDescription(sdp=sdp_offer, type="offer")
        await self.pc.setRemoteDescription(offer)
        
        # answer ìƒì„±
        answer = await self.pc.createAnswer()
        await self.pc.setLocalDescription(answer)
        
        logger.info("âœ… WebRTC answer ìƒì„± ì™„ë£Œ")
        return self.pc.localDescription.sdp
    
    async def _wait_for_connection(self):
        """ì—°ê²° ì™„ë£Œ ëŒ€ê¸° (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)"""
        try:
            # ICE ì—°ê²° ì™„ë£Œ ëŒ€ê¸° (ìµœëŒ€ 10ì´ˆ)
            for _ in range(100):  # 100 * 0.1ì´ˆ = 10ì´ˆ
                if self.pc and self.pc.connectionState == "connected":
                    logger.info("âœ… WebRTC connection established")
                    break
                await asyncio.sleep(0.1)
        except Exception as e:
            logger.error(f"Error waiting for connection: {e}")
    
    def _setup_datachannel_handlers(self):
        """DataChannel ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì„¤ì •"""
        @self.pc.on("datachannel")
        def on_datachannel(channel: RTCDataChannel):
            logger.info(f"DataChannel received: {channel.label}")
            self.data_channel = channel
            
            @channel.on("message")
            def on_message(message):
                try:
                    if isinstance(message, str):
                        data = json.loads(message)
                        logger.debug(f"DataChannel message received: {data}")
                        # í•„ìš”ì‹œ ë©”ì‹œì§€ ì²˜ë¦¬ (ì˜ˆ: ICE candidates ë“±)
                except Exception as e:
                    logger.error(f"Error processing DataChannel message: {e}")
            
            @channel.on("open")
            def on_open():
                logger.info("âœ… DataChannel opened")
            
            @channel.on("close")
            def on_close():
                logger.info("DataChannel closed")
    
    async def _send_datachannel_message(self, message: dict):
        """DataChannelë¡œ ë©”ì‹œì§€ ì „ì†¡"""
        if self.data_channel and self.data_channel.readyState == "open":
            try:
                message_str = json.dumps(message)
                self.data_channel.send(message_str)
            except Exception as e:
                logger.error(f"Error sending DataChannel message: {e}")
        else:
            logger.warning("DataChannel is not open, cannot send message")
    
    async def _handle_signaling(self, message: dict):
        """WebRTC ì‹œê·¸ë„ë§ ë©”ì‹œì§€ ì²˜ë¦¬ (WebSocket ë°©ì‹ìš©)"""
        msg_type = message.get("type")
        
        if msg_type == "offer":
            # í´ë¼ì´ì–¸íŠ¸ë¡œë¶€í„° offer ë°›ìŒ
            offer = RTCSessionDescription(
                sdp=message["sdp"],
                type="offer"
            )
            await self.pc.setRemoteDescription(offer)
            
            # answer ìƒì„±
            answer = await self.pc.createAnswer()
            await self.pc.setLocalDescription(answer)
            
            # answer ì „ì†¡ (WebSocket)
            if self.websocket:
                await self.websocket.send_json({
                    "type": "answer",
                    "sdp": self.pc.localDescription.sdp
                })
            
        elif msg_type == "ice-candidate":
            # ICE candidate ì²˜ë¦¬
            await self._handle_ice_candidate(message)
    
    async def _on_speech_end(self, audio_buffer: np.ndarray):
        """ì‚¬ìš©ìê°€ ë§ì„ ëëƒ„ (VAD ë¬´ìŒ 400ms ê°ì§€) - ë§ ëì—ì„œë§Œ STT í˜¸ì¶œ"""
        logger.info(f"ğŸ¤ Speech ended, audio length: {len(audio_buffer)} samples")
        
        # TODO: STT ì²˜ë¦¬ (ë‹¤ìŒ ë‹¨ê³„ì—ì„œ í™œì„±í™”)
        # try:
        #     transcript = await self.stt_service.transcribe(audio_buffer)
        #     if not transcript or transcript.strip() == "":
        #         logger.warning("Empty transcript")
        #         return
        #     
        #     logger.info(f"STT result: {transcript}")
        #     
        #     # í´ë¼ì´ì–¸íŠ¸ì— transcript ì „ì†¡ (DataChannel)
        #     await self._send_datachannel_message({
        #         "type": "transcript",
        #         "transcript": transcript
        #     })
        #     
        #     # LLM ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
        #     await self._process_llm_response(transcript)
        #     
        # except Exception as e:
        #     logger.error(f"Error processing speech: {e}", exc_info=True)
    
    # TODO: ë‹¤ìŒ ë‹¨ê³„ì—ì„œ í™œì„±í™”
    # async def _process_llm_response(self, user_message: str):
    #     """LLM ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬"""
    #     self.is_speaking = True
    #     text_buffer = ""
    #     sentence_buffer = ""
    #     
    #     try:
    #         # LLM ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­
    #         async for token in self.llm_service.stream_response(user_message):
    #             if self.current_turn_cancelled:
    #                 break
    #             
    #             text_buffer += token
    #             sentence_buffer += token
    #             
    #             # ë¬¸ì¥ ë¶„í•  ê·œì¹™ í™•ì¸
    #             if self._is_sentence_complete(sentence_buffer):
    #                 # ë¬¸ì¥ ì™„ì„± â†’ TTSë¡œ ì „ë‹¬
    #                 sentence = sentence_buffer.strip()
    #                 if sentence:
    #                     await self._send_to_tts(sentence)
    #                 sentence_buffer = ""
    #         
    #         # ë‚¨ì€ í…ìŠ¤íŠ¸ ì²˜ë¦¬
    #         if sentence_buffer.strip() and not self.current_turn_cancelled:
    #             await self._send_to_tts(sentence_buffer.strip())
    #             
    #     except Exception as e:
    #         logger.error(f"LLM processing error: {e}", exc_info=True)
    #     finally:
    #         self.is_speaking = False
    #         # í´ë¼ì´ì–¸íŠ¸ì— idle phase ì „ì†¡
    #         await self._send_datachannel_message({
    #             "type": "phase",
    #             "phase": "idle"
    #         })
    # 
    # def _is_sentence_complete(self, text: str) -> bool:
    #     """ë¬¸ì¥ ì™„ì„± ì—¬ë¶€ íŒë‹¨"""
    #     if len(text) < 20:  # ìµœì†Œ ê¸¸ì´
    #         return False
    #     
    #     # ë¬¸ì¥ ê²½ê³„ í™•ì¸: . ? ! â€¦ \n
    #     sentence_endings = ['.', '?', '!', 'â€¦', '\n']
    #     if any(text.rstrip().endswith(ending) for ending in sentence_endings):
    #         return True
    #     
    #     # ìµœëŒ€ ê¸¸ì´ ì´ˆê³¼ ì‹œ ê°•ì œ ë¶„í• 
    #     if len(text) > 200:
    #         # ë§ˆì§€ë§‰ ê³µë°±ì´ë‚˜ êµ¬ë‘ì ì—ì„œ ë¶„í• 
    #         for i in range(len(text) - 1, max(0, len(text) - 50), -1):
    #             if text[i] in [' ', '.', ',', '!', '?']:
    #                 return True
    #         return True
    #     
    #     return False
    # 
    # async def _send_to_tts(self, text: str):
    #     """TTSë¡œ í…ìŠ¤íŠ¸ ì „ë‹¬ ë° ìŠ¤íŠ¸ë¦¬ë°"""
    #     try:
    #         # TTSê°€ ë¹„í™œì„±í™”ëœ ê²½ìš° ë¡œê·¸ë§Œ ì¶œë ¥
    #         if not hasattr(self.tts_service, 'enabled') or not self.tts_service.enabled:
    #             logger.warning(f"TTS disabled. Would say: {text}")
    #             return
    #         
    #         # ElevenLabs TTS ìŠ¤íŠ¸ë¦¬ë°
    #         async for audio_chunk in self.tts_service.stream_synthesize(text):
    #             if self.current_turn_cancelled:
    #                 break
    #             
    #             # ì˜¤ë””ì˜¤ë¥¼ WebRTC í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (16kHz â†’ 48kHz ë¦¬ìƒ˜í”Œë§)
    #             webrtc_frame = await self.audio_processor.prepare_output_frame(audio_chunk)
    #             
    #             # WebRTCë¡œ ì†¡ì¶œ
    #             if self.audio_sender:
    #                 await self.audio_sender.push_audio(webrtc_frame)
    #                 
    #     except Exception as e:
    #         logger.error(f"TTS error: {e}", exc_info=True)
    # 
    # async def _cancel_current_turn(self):
    #     """í˜„ì¬ í„´ ì·¨ì†Œ (Barge-in)"""
    #     logger.info("Cancelling current turn")
    #     # LLM/TTS ì‘ì—… ì·¨ì†ŒëŠ” í”Œë˜ê·¸ë¡œ ì²˜ë¦¬ (ì‹¤ì œ ì·¨ì†ŒëŠ” ê° ì„œë¹„ìŠ¤ì—ì„œ ì²˜ë¦¬)
    #     # ì˜¤ë””ì˜¤ í flush
    #     if self.audio_sender:
    #         # í ë¹„ìš°ê¸°
    #         while not self.audio_sender._queue.empty():
    #             try:
    #                 self.audio_sender._queue.get_nowait()
    #             except:
    #                 break
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info(f"Cleaning up session: {self.session_id}")
        
        try:
            # TODO: ë‹¤ìŒ ë‹¨ê³„ì—ì„œ í™œì„±í™”
            # VAD ì •ë¦¬
            # await self.vad_processor.cleanup()
            pass
        except Exception as e:
            logger.warning(f"Error cleaning up VAD processor: {e}")
        
        # WebRTC ì—°ê²° ì¢…ë£Œ
        if self.pc:
            try:
                # ì—°ê²° ìƒíƒœ í™•ì¸ í›„ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ
                if self.pc.connectionState != "closed":
                    await self.pc.close()
            except Exception as e:
                logger.warning(f"Error closing PeerConnection: {e}")
            finally:
                self.pc = None
        
        # ì˜¤ë””ì˜¤ ì†¡ì¶œ íŠ¸ë™ ì¢…ë£Œ
        if self.audio_sender:
            try:
                self.audio_sender.close()
            except Exception as e:
                logger.warning(f"Error closing audio sender: {e}")
            finally:
                self.audio_sender = None

