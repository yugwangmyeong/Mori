part of '../realtime_service.dart';

// ë©”ì‹œì§€ ì²˜ë¦¬ ê´€ë ¨ ë¡œì§
extension RealtimeMessageHandler on RealtimeService {
  // ë©”ì‹œì§€ ì²˜ë¦¬
  void _handleMessage(Map<String, dynamic> message) {
    final type = message['type'] as String?;
    
    switch (type) {
      case 'session.created':
        _logEvent('session.created');
        _sessionReady = false;
        // session.created ì§í›„ í†µí™” ì„¤ì • session.update ì „ì†¡ (session.updated ìˆ˜ì‹  í›„ ready)
        _sendSessionUpdate();
        break;
      case 'session.update':
        _logEvent('session.update');
        break;
      case 'session.updated':
        _logEvent('session.updated');
        _sessionReady = true;
        _logFlow('sessionReady=true');
        _updateConversationReady();
        break;
      case 'conversation.item.input_audio_transcription.completed':
        final transcript = message['item']?['input_audio_transcription']?['transcript'] ?? 
                          message['transcript'] ?? '';
        if (transcript.isNotEmpty) {
          final preview = transcript.length > 30 ? transcript.substring(0, 30) : transcript;
          _logMic('TRANSCRIBE completed len=${transcript.length} preview="$preview"');
          _userTranscriptController.add(transcript);
        } else {
          _logMic('TRANSCRIBE completed len=0');
        }
        break;
      case 'conversation.item.input_audio_transcription.failed':
        _logEvent('conversation.item.input_audio_transcription.failed', data: {'error': message['error']});
        break;
      case 'conversation.item.created':
        _logEvent('conversation.item.created');
        break;
      case 'conversation.item.input_audio_transcription.started':
        _logEvent('conversation.item.input_audio_transcription.started');
        break;
      case 'conversation.item.input_audio_transcription.updated':
        _logEvent('conversation.item.input_audio_transcription.updated');
        break;
      case 'conversation.item.done':
        _logEvent('conversation.item.done');
        // create_response=falseì¼ ë•Œ ìˆ˜ë™ìœ¼ë¡œ response.create ì „ì†¡
        if (!_isPaused && !_responseInFlight && isConversationReady) {
          createResponse();
        }
        break;
      case 'input_audio_buffer.committed':
        _logMic('VAD committed');
        _updateUiPhase(UiPhase.thinking);
        if (!_isPaused && !_responseInFlight && isConversationReady) {
          createResponse();
        }
        break;
      case 'input_audio_buffer.speech_stopped':
      case 'speech_stopped':
        if (_isPaused) {
          _logMic('VAD speech_stopped (ignored: paused)');
          return;
        }
        _logMic('VAD speech_stopped');
        // create_response=trueì´ë©´ ìë™ ì‘ë‹µ ìƒì„±ë˜ë¯€ë¡œ ìˆ˜ë™ í˜¸ì¶œ ë¶ˆí•„ìš”
        // create_response=falseì¼ ë•Œë§Œ ë””ë°”ìš´ìŠ¤ í›„ ìˆ˜ë™ í˜¸ì¶œ
        // í˜„ì¬ëŠ” create_response=trueì´ë¯€ë¡œ ì´ ë¶€ë¶„ì€ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì œê±° ê°€ëŠ¥
        // _speechStopDebounce?.cancel();
        // _speechStopDebounce = Timer(const Duration(milliseconds: 250), () {
        //   if (_responseInFlight) {
        //     _logEvent('response.create ignored (in flight)');
        //     return;
        //   }
        //   createResponse();
        // });
        break;
      case 'input_audio_buffer.speech_started':
      case 'speech_started':
        if (_isPaused) {
          _logMic('VAD speech_started (ignored: paused)');
          return;
        }
        _logMic('VAD speech_started');
        _updateUiPhase(UiPhase.listening);
        break;
      case 'response.created':
        _logEvent('response.created');
        _updateUiPhase(UiPhase.speaking);
        break;
      case 'response.output_item.added':
        _logEvent('response.output_item.added');
        break;
      case 'response.output_item.done':
        _logEvent('response.output_item.done');
        break;
      case 'response.audio_transcript.delta':
        final delta = message['delta'] ?? '';
        if (delta.isNotEmpty) {
          _aiResponseTextController.add(delta);
        }
        break;
      case 'response.audio_transcript.done':
        _logEvent('response.audio_transcript.done');
        break;
      case 'response.audio.delta':
        _logNoisy('response.audio.delta');
        break;
      case 'response.done':
        _logEvent('response.done');
        _responseInFlight = false;
        _logFlow('responseInFlight=false');
        _updateUiPhase(UiPhase.idle);
        break;
      case 'error':
        _sessionReady = false;
        _logEvent('error', data: {'error': message['error']});
        if (_lastSessionUpdatePayload != null) {
          _logEvent('last session.update payload', data: _lastSessionUpdatePayload);
        }
        _updateConversationReady();
        break;
      
      // Python ë°±ì—”ë“œ ë©”ì‹œì§€ ì²˜ë¦¬
      case 'vad.speech_started':
        _logMic('VAD speech_started (Python backend)');
        _updateUiPhase(UiPhase.listening);
        break;
      case 'vad.speech_stopped':
        _logMic('VAD speech_stopped (Python backend)');
        _updateUiPhase(UiPhase.thinking);
        break;
      case 'stt.partial':
        final delta = message['delta'] ?? message['text'] ?? '';
        if (delta.isNotEmpty) {
          _logMic('STT partial: $delta');
        }
        break;
      case 'stt.final':
        final text = message['text'] ?? '';
        if (text.isNotEmpty) {
          _logMic('STT final: $text');
          _userTranscriptController.add(text);
        }
        break;
      case 'llm.response':
        final text = message['text'] ?? '';
        if (text.isNotEmpty) {
          _logEvent('LLM response: $text');
          _aiResponseTextController.add(text);
        }
        break;
      case 'tts.start':
        print('ğŸ”Š [TTS] Received tts.start');
        _handleTtsStart(message);
        break;
      case 'tts.chunk':
        print('ğŸ”Š [TTS] Received tts.chunk');
        _handleTtsChunk(message);
        break;
      case 'tts.end':
        print('ğŸ”Š [TTS] Received tts.end');
        _handleTtsEnd(message);
        break;
      
      default:
        _logNoisy('message: $type');
    }
  }
  
  // TTS ì²˜ë¦¬ ê´€ë ¨ ë©”ì„œë“œ (ë²„í¼ëŠ” RealtimeService í´ë˜ìŠ¤ì— ì¶”ê°€ í•„ìš”)
  
  void _handleTtsStart(Map<String, dynamic> message) {
    final turnId = message['turn_id'] as int?;
    final totalBytes = message['total_bytes'] as int?;
    
    print('ğŸ”Š [TTS START] turn_id=$turnId, total_bytes=$totalBytes');
    
    if (turnId != null) {
      _ttsBuffers[turnId] = [];
      _logEvent('TTS start: turn_id=$turnId, bytes=$totalBytes');
      print('   â†’ Buffer initialized for turn $turnId');
    } else {
      print('   âš ï¸ turn_id is null!');
    }
  }
  
  void _handleTtsChunk(Map<String, dynamic> message) {
    final turnId = message['turn_id'] as int?;
    final audioB64 = message['audio_b64'] as String?;
    
    print('ğŸ”Š [TTS CHUNK] turn_id=$turnId, has_audio=${audioB64 != null}, b64_length=${audioB64?.length}');
    
    if (turnId != null && audioB64 != null) {
      try {
        final bytes = base64Decode(audioB64);
        _ttsBuffers[turnId]?.add(bytes);
        final bufferSize = _ttsBuffers[turnId]?.length ?? 0;
        print('   â†’ Chunk added: ${bytes.length} bytes, buffer_chunks=$bufferSize');
        _logNoisy('TTS chunk received: turn_id=$turnId, bytes=${bytes.length}');
      } catch (e) {
        print('   âŒ Base64 decode error: $e');
        _logEvent('TTS chunk decode error', data: {'error': e.toString()});
      }
    } else {
      print('   âš ï¸ Missing data: turn_id=$turnId, audio_b64=${audioB64 != null}');
    }
  }
  
  void _handleTtsEnd(Map<String, dynamic> message) async {
    final turnId = message['turn_id'] as int?;
    
    print('ğŸ”Š [TTS END] turn_id=$turnId, has_buffer=${_ttsBuffers.containsKey(turnId)}');
    
    if (turnId != null && _ttsBuffers.containsKey(turnId)) {
      try {
        // ëª¨ë“  ì²­í¬ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
        final allChunks = _ttsBuffers[turnId]!;
        final chunkCount = allChunks.length;
        final totalLength = allChunks.fold<int>(0, (sum, chunk) => sum + chunk.length);
        
        print('   â†’ Merging $chunkCount chunks, total_bytes=$totalLength');
        
        final mp3Bytes = Uint8List(totalLength);
        
        int offset = 0;
        for (final chunk in allChunks) {
          mp3Bytes.setRange(offset, offset + chunk.length, chunk);
          offset += chunk.length;
        }
        
        print('   â†’ MP3 merged successfully: ${mp3Bytes.length} bytes');
        _logEvent('TTS complete: turn_id=$turnId, total_bytes=${mp3Bytes.length}');
        
        // MP3 íŒŒì¼ë¡œ ì €ì¥ ë° ì¬ìƒ
        print('   â†’ Calling _playTtsAudio...');
        await _playTtsAudio(mp3Bytes, turnId);
        
        // ë²„í¼ ì •ë¦¬
        _ttsBuffers.remove(turnId);
        print('   âœ… TTS buffer cleaned');
      } catch (e) {
        print('   âŒ TTS end error: $e');
        _logEvent('TTS end error', data: {'error': e.toString()});
      }
    } else {
      print('   âš ï¸ No buffer found for turn $turnId');
    }
  }
  
  Future<void> _playTtsAudio(Uint8List mp3Bytes, int turnId) async {
    try {
      print('ğŸ”Š [_playTtsAudio] Starting, bytes=${mp3Bytes.length}');
      
      // ì„ì‹œ ë””ë ‰í† ë¦¬ì— MP3 íŒŒì¼ ì €ì¥
      final directory = await getTemporaryDirectory();
      final filePath = '${directory.path}/tts_$turnId.mp3';
      final file = File(filePath);
      
      print('   â†’ Saving to: $filePath');
      await file.writeAsBytes(mp3Bytes);
      print('   â†’ File saved successfully');
      
      _logEvent('TTS saved to: $filePath');
      
      // audioplayersë¡œ ì¬ìƒ
      print('   â†’ Calling playTtsFile...');
      await playTtsFile(filePath);
      
      print('ğŸ”Š TTS ì¬ìƒ ìš”ì²­ ì™„ë£Œ: $filePath');
    } catch (e) {
      print('âŒ [_playTtsAudio] Error: $e');
      _logEvent('TTS play error', data: {'error': e.toString()});
    }
  }

  // ì‘ë‹µ ìƒì„± ìš”ì²­
  void createResponse() {
    if (!isConversationReady) {
      _logFlow('response.create blocked (ready=$isConversationReady)');
      return;
    }

    if (_isPaused) {
      _logEvent('response.create skipped (paused)');
      return;
    }

    // ì´ë¯¸ ì‘ë‹µì´ ì§„í–‰ ì¤‘ì´ë©´ ë¬´ì‹œ
    if (_responseInFlight) {
      _logEvent('response.create ignored (in flight)');
      return;
    }

    _responseInFlight = true;
    _logFlow('responseInFlight=true');
    _sendMessage({
      'type': 'response.create',
    });
    _logEvent('response.create sent');
  }

  // ì„¸ì…˜ ì—…ë°ì´íŠ¸ ì „ì†¡ (í†µí™” ì„¤ì •)
  void _sendSessionUpdate() {
    final payload = {
      'type': 'session.update',
      'session': {
        'type': 'realtime',
        'model': 'gpt-realtime',
        'instructions': 'ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ëª¨ë“  ëŒ€í™”ëŠ” í•œêµ­ì–´ë¡œ ì§„í–‰ë©ë‹ˆë‹¤. ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•œ í†¤ìœ¼ë¡œ ëŒ€í™”í•˜ì„¸ìš”.',
        'audio': {
          'input': {
            'turn_detection': {
              'type': 'server_vad',
              'threshold': 0.5,
              'prefix_padding_ms': 300,
              'silence_duration_ms': 800,
              'create_response': true,
            },
            'transcription': {
              'model': 'gpt-4o-mini-transcribe',
            },
          },
        },
      },
    };

    _lastSessionUpdatePayload = payload;
    _sessionReady = false;
    _logEvent('session.update send', data: payload);
    _sendMessage(payload);
  }

  // ë©”ì‹œì§€ ì „ì†¡ (DataChannel)
  void _sendMessage(Map<String, dynamic> message) {
    if (_dataChannel == null || _dataChannel!.state != RTCDataChannelState.RTCDataChannelOpen) {
      _logFlow('message send blocked: dataChannel not open');
      return;
    }

    try {
      _dataChannel!.send(RTCDataChannelMessage(jsonEncode(message)));
    } catch (e) {
      _logEvent('send error', data: {'error': e.toString()});
    }
  }
}

