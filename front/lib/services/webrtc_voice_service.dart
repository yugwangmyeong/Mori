/// WebRTC ê¸°ë°˜ Voice AI ì„œë¹„ìŠ¤
/// HTTP POSTë¡œ ì‹œê·¸ë„ë§í•˜ê³  DataChannelë¡œ ë©”ì‹œì§€ êµí™˜
import 'dart:async';
import 'dart:convert';
import 'package:flutter/foundation.dart';
import 'package:flutter_dotenv/flutter_dotenv.dart';
import 'package:flutter_webrtc/flutter_webrtc.dart';
import 'package:http/http.dart' as http;
import '../models/ui_phase.dart';

class WebRTCVoiceService {
  RTCPeerConnection? _peerConnection;
  RTCDataChannel? _dataChannel;
  MediaStream? _localStream;
  MediaStream? _remoteStream;
  List<MediaStreamTrack>? _audioTracks; // ì˜¤ë””ì˜¤ íŠ¸ë™ ì €ì¥
  RTCRtpSender? _audioSender; // ì˜¤ë””ì˜¤ sender ì €ì¥ (ì¤‘ë³µ ë°©ì§€)
  String? _audioTrackId; // ì˜¤ë””ì˜¤ íŠ¸ë™ ID ì €ì¥ (sender ì°¾ê¸°ìš©)
  
  bool _isConnected = false;
  String? _sessionId;
  bool _isMicEnabled = true; // ë§ˆì´í¬ í™œì„±í™” ìƒíƒœ
  
  // ìŠ¤íŠ¸ë¦¼ ì»¨íŠ¸ë¡¤ëŸ¬
  final StreamController<UiPhase> _uiPhaseController = 
      StreamController<UiPhase>.broadcast();
  final StreamController<String> _connectionStatusController = 
      StreamController<String>.broadcast();
  final StreamController<String> _transcriptController = 
      StreamController<String>.broadcast();
  final StreamController<bool> _micEnabledController = 
      StreamController<bool>.broadcast();
  
  UiPhase _currentPhase = UiPhase.idle;
  
  // ì„œë²„ URL
  static String get _baseUrl {
    if (kDebugMode) {
      final serverIp = _getServerBaseUrl();
      return 'http://$serverIp:8000';
    }
    return 'https://your-production-server.com';
  }
  
  static String _getServerBaseUrl() {
    final envUrl = dotenv.get('BACKEND_URL', fallback: '').trim();
    
    // .envì— BACKEND_URLì´ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©´ ì‚¬ìš©
    if (envUrl.isNotEmpty) {
      if (envUrl.contains(':')) {
        return envUrl.split(':').first;
      }
      return envUrl;
    }
    
    // ê¸°ë³¸ê°’: ë°±ì—”ë“œ ì„œë²„ IP
    return '172.30.1.29';
  }
  
  // Getters
  Stream<UiPhase> get uiPhase => _uiPhaseController.stream;
  Stream<String> get connectionStatus => _connectionStatusController.stream;
  Stream<String> get transcript => _transcriptController.stream;
  Stream<bool> get micEnabled => _micEnabledController.stream;
  bool get isConnected => _isConnected;
  bool get isMicEnabled => _isMicEnabled;
  UiPhase get currentPhase => _currentPhase;
  
  void _updatePhase(UiPhase phase) {
    if (_currentPhase != phase) {
      _currentPhase = phase;
      _uiPhaseController.add(phase);
    }
  }
  
  /// WebRTC ì—°ê²° ì‹œì‘ (HTTP POST + DataChannel)
  Future<void> connect() async {
    try {
      _sessionId = 'session_${DateTime.now().millisecondsSinceEpoch}';
      
      print('ğŸ”Œ [WebRTC] ì—°ê²° ì‹œë„: $_baseUrl/api/realtime/calls');
      print('ğŸ”Œ [WebRTC] ì„œë²„ IP: ${_getServerBaseUrl()}');
      print('ğŸ”Œ [WebRTC] í¬íŠ¸: 8000');
      
      // WebRTC PeerConnection ìƒì„±
      await _createPeerConnection();
      
      // ë¡œì»¬ ë¯¸ë””ì–´ ìŠ¤íŠ¸ë¦¼ ê°€ì ¸ì˜¤ê¸° (ë§ˆì´í¬)
      _localStream = await navigator.mediaDevices.getUserMedia({
        'audio': {
          'echoCancellation': true,
          'noiseSuppression': true,
          'autoGainControl': true,
        }
      });
      
      // ì˜¤ë””ì˜¤ íŠ¸ë™ ì €ì¥
      _audioTracks = _localStream!.getAudioTracks();
      
      // ë¡œì»¬ ìŠ¤íŠ¸ë¦¼ì„ PeerConnectionì— ì¶”ê°€
      for (final track in _audioTracks!) {
        await _peerConnection!.addTrack(track, _localStream!);
      }
      
      // sender ê°œìˆ˜ í™•ì¸ ë° ì €ì¥
      final allSenders = await _peerConnection!.getSenders();
      final audioSenders = allSenders.where((s) => s.track?.kind == 'audio').toList();
      print('ğŸ“Š ì´ sender ê°œìˆ˜: ${allSenders.length}, ì˜¤ë””ì˜¤ sender ê°œìˆ˜: ${audioSenders.length}');
      
      // ëª¨ë“  sender ì •ë³´ ì¶œë ¥
      for (var i = 0; i < allSenders.length; i++) {
        final sender = allSenders[i];
        print('   - sender[$i]: track=${sender.track?.id ?? "null"}, kind=${sender.track?.kind ?? "null"}');
      }
      
      if (audioSenders.length > 1) {
        print('âš ï¸ ê²½ê³ : ì˜¤ë””ì˜¤ senderê°€ ${audioSenders.length}ê°œì…ë‹ˆë‹¤! ì¤‘ë³µ ê°€ëŠ¥ì„± ìˆìŒ');
      }
      if (audioSenders.isNotEmpty) {
        _audioSender = audioSenders.first;
        _audioTrackId = _audioSender!.track?.id;
        print('ğŸ¤ ì˜¤ë””ì˜¤ sender ì €ì¥: track=${_audioTrackId ?? "null"}');
      }
      
      // DataChannel ìƒì„±
      await _createDataChannel();
      
      // Offer ìƒì„±
      final offer = await _peerConnection!.createOffer();
      await _peerConnection!.setLocalDescription(offer);
      
      // HTTP POSTë¡œ offer ì „ì†¡
      final response = await http.post(
        Uri.parse('$_baseUrl/api/realtime/calls'),
        headers: {
          'Content-Type': 'application/sdp',
        },
        body: offer.sdp,
      );
      
      if (response.statusCode != 200 && response.statusCode != 201) {
        throw Exception('Offer ì „ì†¡ ì‹¤íŒ¨: ${response.statusCode} - ${response.body}');
      }
      
      // Session ID ì €ì¥
      final sessionIdHeader = response.headers['x-session-id'] ?? response.headers['X-Session-Id'];
      if (sessionIdHeader != null) {
        _sessionId = sessionIdHeader;
        print('âœ… Session ID ì €ì¥: $_sessionId');
      }
      
      // Answer SDP ë°›ê¸°
      final answerSdp = response.body;
      
      // Answer ì„¤ì •
      final answer = RTCSessionDescription(answerSdp, 'answer');
      await _peerConnection!.setRemoteDescription(answer);
      
      _updateConnectionStatus('connecting');
      _updatePhase(UiPhase.idle);
      
    } catch (e) {
      print('Connection error: $e');
      _updateConnectionStatus('error');
      rethrow;
    }
  }
  
  /// DataChannel ìƒì„±
  Future<void> _createDataChannel() async {
    final dataChannelInit = RTCDataChannelInit();
    dataChannelInit.ordered = true;
    _dataChannel = await _peerConnection!.createDataChannel('messages', dataChannelInit);
    
    // DataChannel ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
    _dataChannel!.onDataChannelState = (RTCDataChannelState state) {
      print('DataChannel state: $state');
      if (state == RTCDataChannelState.RTCDataChannelOpen) {
        print('âœ… DataChannel opened');
      }
    };
    
    _dataChannel!.onMessage = (RTCDataChannelMessage message) {
      try {
        final data = jsonDecode(message.text) as Map<String, dynamic>;
        _handleDataChannelMessage(data);
      } catch (e) {
        print('Error handling DataChannel message: $e');
      }
    };
  }
  
  /// WebRTC PeerConnection ìƒì„±
  Future<void> _createPeerConnection() async {
    final configuration = {
      'iceServers': [
        {'urls': 'stun:stun.l.google.com:19302'},
      ],
    };
    
    _peerConnection = await createPeerConnection(configuration);
    
    // ì—°ê²° ìƒíƒœ ë³€ê²½
    _peerConnection!.onConnectionState = (RTCPeerConnectionState state) {
      print('PeerConnection state: $state');
      if (state == RTCPeerConnectionState.RTCPeerConnectionStateConnected) {
        _isConnected = true;
        _updateConnectionStatus('connected');
      } else if (state == RTCPeerConnectionState.RTCPeerConnectionStateDisconnected ||
                 state == RTCPeerConnectionState.RTCPeerConnectionStateFailed) {
        _isConnected = false;
        _updateConnectionStatus('disconnected');
      }
    };
    
    // ì›ê²© íŠ¸ë™ ìˆ˜ì‹ 
    _peerConnection!.onTrack = (RTCTrackEvent event) {
      if (event.track.kind == 'audio') {
        print('Remote audio track received');
        _remoteStream = event.streams[0];
        _updatePhase(UiPhase.speaking);
        
        // ì˜¤ë””ì˜¤ ì¬ìƒ (ìë™ìœ¼ë¡œ ì¬ìƒë¨)
      }
    };
  }
  
  /// DataChannel ë©”ì‹œì§€ ì²˜ë¦¬
  void _handleDataChannelMessage(Map<String, dynamic> data) {
    final type = data['type'] as String?;
    
    if (type == 'transcript') {
      // STT transcript ìˆ˜ì‹ 
      final transcript = data['transcript'] as String?;
      if (transcript != null && transcript.isNotEmpty) {
        _transcriptController.add(transcript);
      }
    } else if (type == 'phase') {
      // UI phase ì—…ë°ì´íŠ¸ ìˆ˜ì‹ 
      final phaseStr = data['phase'] as String?;
      if (phaseStr != null) {
        UiPhase? phase;
        switch (phaseStr) {
          case 'idle':
            phase = UiPhase.idle;
            break;
          case 'listening':
            phase = UiPhase.listening;
            break;
          case 'speaking':
            phase = UiPhase.speaking;
            break;
        }
        if (phase != null) {
          _updatePhase(phase);
        }
      }
    }
  }
  
  void _updateConnectionStatus(String status) {
    _connectionStatusController.add(status);
  }
  
  /// ë§ˆì´í¬ í† ê¸€ (ì¼œê¸°/ë„ê¸°)
  Future<void> toggleMicrophone() async {
    if (_peerConnection == null) {
      print('âš ï¸ PeerConnectionì´ ì—†ìŠµë‹ˆë‹¤.');
      return;
    }
    
    // sender í™•ì¸ ë° ì €ì¥
    if (_audioSender == null) {
      final senders = await _peerConnection!.getSenders();
      
      print('ğŸ“Š í˜„ì¬ sender ìƒíƒœ:');
      print('   - ì´ sender ê°œìˆ˜: ${senders.length}');
      
      // ëª¨ë“  sender ì •ë³´ ì¶œë ¥
      for (var i = 0; i < senders.length; i++) {
        final sender = senders[i];
        print('   - sender[$i]: track=${sender.track?.id ?? "null"}, kind=${sender.track?.kind ?? "null"}');
      }
      
      // track IDë¡œ sender ì°¾ê¸° (ì´ì „ì— ì €ì¥í•œ track IDê°€ ìˆìœ¼ë©´)
      if (_audioTrackId != null) {
        _audioSender = senders.firstWhere(
          (s) => s.track?.id == _audioTrackId,
          orElse: () => senders.firstWhere(
            (s) => s.track?.kind == 'audio',
            orElse: () => throw Exception('No audio sender found'),
          ),
        );
        print('ğŸ¤ ì €ì¥ëœ track IDë¡œ sender ì°¾ê¸°: ${_audioTrackId}');
      } else {
        // track IDê°€ ì—†ìœ¼ë©´ kindë¡œ ì°¾ê¸°
        final audioSenders = senders.where((s) => s.track?.kind == 'audio').toList();
        if (audioSenders.isEmpty) {
          print('âŒ ì˜¤ë””ì˜¤ senderë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
          return;
        }
        if (audioSenders.length > 1) {
          print('âš ï¸ ê²½ê³ : ì˜¤ë””ì˜¤ senderê°€ ${audioSenders.length}ê°œì…ë‹ˆë‹¤! ì²« ë²ˆì§¸ sender ì‚¬ìš©');
        }
        _audioSender = audioSenders.first;
        _audioTrackId = _audioSender!.track?.id;
      }
      
      print('ğŸ¤ ì˜¤ë””ì˜¤ sender ì €ì¥: track=${_audioTrackId ?? "null"}');
    }
    
    _isMicEnabled = !_isMicEnabled;
    
    // í˜„ì¬ sender ìƒíƒœ í™•ì¸
    print('ğŸ” replaceTrack ì „ ìƒíƒœ:');
    print('   - sender.track: ${_audioSender!.track?.id ?? "null"}');
    print('   - sender.track?.kind: ${_audioSender!.track?.kind ?? "null"}');
    print('   - sender.track?.enabled: ${_audioSender!.track?.enabled ?? "null"}');
    
    if (_isMicEnabled) {
      // ë§ˆì´í¬ ì¼œê¸°: ìƒˆ íŠ¸ë™ì„ ê°€ì ¸ì™€ì„œ êµì²´
      if (_localStream == null) {
        // ìŠ¤íŠ¸ë¦¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ê°€ì ¸ì˜¤ê¸°
        _localStream = await navigator.mediaDevices.getUserMedia({
          'audio': {
            'echoCancellation': true,
            'noiseSuppression': true,
            'autoGainControl': true,
          }
        });
        _audioTracks = _localStream!.getAudioTracks();
      }
      
      // íŠ¸ë™ì„ PeerConnectionì— êµì²´
      if (_audioTracks != null && _audioTracks!.isNotEmpty) {
        await _audioSender!.replaceTrack(_audioTracks!.first);
        _audioTrackId = _audioTracks!.first.id; // track ID ì—…ë°ì´íŠ¸
        print('ğŸ¤ ë§ˆì´í¬ ON: ${_audioTracks!.first.id}');
        print('   - replaceTrack í›„ sender.track: ${_audioSender!.track?.id ?? "null"}');
      }
    } else {
      // ë§ˆì´í¬ ë„ê¸°: íŠ¸ë™ì„ nullë¡œ êµì²´í•˜ì—¬ ì˜¤ë””ì˜¤ ì „ì†¡ ì™„ì „ ì¤‘ì§€
      print('ğŸ›‘ replaceTrack(null) í˜¸ì¶œ ì¤‘...');
      await _audioSender!.replaceTrack(null);
      print('âœ… replaceTrack(null) ì™„ë£Œ');
      print('   - replaceTrack í›„ sender.track: ${_audioSender!.track?.id ?? "null"}');
      
      // ë¡œì»¬ íŠ¸ë™ ì¤‘ì§€
      if (_audioTracks != null) {
        _audioTracks!.forEach((track) {
          track.stop();
          print('ğŸ¤ ë§ˆì´í¬ OFF: ${track.id}');
        });
      }
      
      // ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
      _localStream?.getTracks().forEach((track) => track.stop());
      _localStream?.dispose();
      _localStream = null;
      _audioTracks = null;
    }
    
    // ìµœì¢… sender ìƒíƒœ í™•ì¸
    final finalSenders = await _peerConnection!.getSenders();
    final finalAudioSenders = finalSenders.where((s) => s.track?.kind == 'audio').toList();
    print('ğŸ“Š replaceTrack í›„ ìµœì¢… ìƒíƒœ:');
    print('   - ì´ sender ê°œìˆ˜: ${finalSenders.length}');
    print('   - ì˜¤ë””ì˜¤ sender ê°œìˆ˜ (trackì´ ìˆëŠ”): ${finalAudioSenders.length}');
    
    // ëª¨ë“  sender ì •ë³´ ì¶œë ¥ (trackì´ nullì´ì–´ë„)
    for (var i = 0; i < finalSenders.length; i++) {
      final sender = finalSenders[i];
      final isOurSender = sender == _audioSender;
      print('   - sender[$i]: track=${sender.track?.id ?? "null"}, kind=${sender.track?.kind ?? "null"} ${isOurSender ? "â† ìš°ë¦¬ sender" : ""}');
    }
    
    // ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” senderì˜ ìµœì¢… ìƒíƒœ
    if (_audioSender != null) {
      print('   - ìš°ë¦¬ sender.track: ${_audioSender!.track?.id ?? "null"}');
      print('   - ìš°ë¦¬ sender.track?.kind: ${_audioSender!.track?.kind ?? "null"}');
    }
    
    _micEnabledController.add(_isMicEnabled);
    print('âœ… ë§ˆì´í¬ ìƒíƒœ ë³€ê²½: ${_isMicEnabled ? "ON" : "OFF"}');
  }
  
  /// ë§ˆì´í¬ ì¼œê¸°
  Future<void> enableMicrophone() async {
    if (!_isMicEnabled) {
      await toggleMicrophone();
    }
  }
  
  /// ë§ˆì´í¬ ë„ê¸°
  Future<void> disableMicrophone() async {
    if (_isMicEnabled) {
      await toggleMicrophone();
    }
  }
  
  /// ì—°ê²° ì¢…ë£Œ
  Future<void> disconnect() async {
    try {
      // Hangup ìš”ì²­ (ì„ íƒì )
      if (_sessionId != null) {
        try {
          await http.post(
            Uri.parse('$_baseUrl/api/realtime/calls/$_sessionId/hangup'),
          );
        } catch (e) {
          print('Hangup request error (ignored): $e');
        }
      }
      
      // DataChannel ì¢…ë£Œ
      await _dataChannel?.close();
      _dataChannel = null;
      
      // ë¡œì»¬ ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
      _localStream?.getTracks().forEach((track) {
        track.stop();
      });
      _localStream?.dispose();
      _localStream = null;
      
      // ì›ê²© ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
      _remoteStream?.dispose();
      _remoteStream = null;
      
      // PeerConnection ì¢…ë£Œ
      await _peerConnection?.close();
      _peerConnection = null;
      _audioSender = null; // senderë„ ì´ˆê¸°í™”
      _audioTrackId = null; // track IDë„ ì´ˆê¸°í™”
      
      _isConnected = false;
      _sessionId = null;
      _updateConnectionStatus('disconnected');
      _updatePhase(UiPhase.idle);
      
    } catch (e) {
      print('Disconnect error: $e');
    }
  }
  
  /// ë¦¬ì†ŒìŠ¤ ì •ë¦¬
  Future<void> dispose() async {
    await disconnect();
    await _uiPhaseController.close();
    await _connectionStatusController.close();
    await _transcriptController.close();
    await _micEnabledController.close();
  }
}